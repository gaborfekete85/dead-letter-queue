server:
  port: 8756
application:
  configs:
    persistence:
      enabled: true
    topic:
      serviceAgreement: pcmproser_pcoevents_serviceagreement_v2
      serviceAgreementDlt: pcmproser_pcoevents_serviceagreement_v2_dlt
      serviceAgreementRetry: pcmproser_pcoevents_serviceagreement_v2_retry

spring:
  application:
    name: DLT
  datasource:
    url: ${DATASOURCE_URL:jdbc:postgresql://localhost:5432/postgres?currentSchema=public}
    username: ${DATASOURCE_USER:postgres}
    password: ${DATASOURCE_PASSWORD:postgres}
  jpa:
    show-sql: true
    hibernate:
      ddl-auto: none
      naming-strategy: org.hibernate.cfg.ImprovedNamingStrategy
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        format_sql: true
        ddl-auto: none
        jdbc:
          lob:
            non_contextual_creation: true
  flyway:
    enabled: true
    baseline-on-migrate: true
    placeholderReplacement: false

  kafka:
    producer:
      client-id: service-agreement-generator
      bootstrap-servers: localhost:9092
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
      properties:
        schema.registry.url: http://localhost:8081
    consumer:
      client-id: service-agreement-generator
      group-id: process-service-group
      bootstrap-servers: localhost:9092
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.streams.serdes.avro.SpecificAvroDeserializer
      properties:
        schema.registry.url: http://localhost:8081

  cloud:
    stream:
      bindings:
#        notification-input-channel:
#          applicationId: notificationApp
#          destination: avro-pos-topic
#        notification-output-channel:
#          applicationId: notificationOutApp
#          destination: loyalty-topic
        sa-input-channel:
          destination: pcmproser_pcoevents_serviceagreement_v2_dlt
        sa-output-channel:
          destination: pcmproser_pcoevents_serviceagreement_v2_out
        dlt-input-channel:
          destination: pcmproser_pcoevents_serviceagreement_v2_dlt_avro
        dlt-output-channel:
          destination: pcmproser_pcoevents_serviceagreement_v2_dlt_avro_out
#        sa-input-channel_dlt:
#          applicationId: saDltApp
#          destination: pcmproser_pcoevents_serviceagreement_v2_dlt
#        err-outbound-channel:
#          destination: transactionApp-avro-pos-topic
#        hadoop-input-channel:
#          destination: avro-pos-topic
#        hadoop-output-channel:
#          destination: hadoop-sink-topic
      kafka:
        streams:
          applicationId: mainApplication5
          properties:
            commit.interval.ms: 1000
            state.dir: /Users/feketegabor/rocksdb
          binder:
            brokers:  localhost:9092
            serdeError: logAndContinue
            deserializationExceptionHandler: sendToDlq
            configuration:
              default:
                key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
              commit.interval.ms: 1000
              schema.registry.url: http://localhost:8081
          bindings:
#            notification-input-channel:
#              consumer:
#                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
#                dlqName: transactionApp-avro-pos-topic
#                error-channel-enabled: true
#            notification-output-channel:
#              producer:
#                valueSerde: io.confluent.kafka.streams.serdes.json.KafkaJsonSchemaSerde
            sa-input-channel:
              consumer:
                materializedAs: sa-input-store
                applicationId: saInputApplication5
                configuration:
                  group.id: randomGroupswdw
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
                dlqName: pcmproser_pcoevents_serviceagreement_v2_dlt
                error-channel-enabled: true
            sa-output-channel:
              producer:
                applicationId: saOutputApplication
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
            dlt-input-channel:
              consumer:
                materializedAs: dlt-input-store
                applicationId: saDltAvroInputApplication1
                configuration:
                  group.id: randomGroup123
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
            dlt-output-channel:
              producer:
                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
#            hadoop-input-channel:
#              consumer:
#                valueSerde: io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde
#            hadoop-output-channel:
#              producer:
#                valueSerde: io.confluent.kafka.streams.serdes.json.KafkaJsonSchemaSerde

default:
  deserialization:
    exception:
      handler: sendToDlq